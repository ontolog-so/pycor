{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --upgrade pip\n",
    "#!pip install beautifulsoup4\n",
    "#!pip install feedparser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## wikipedia 한국어 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import csv\n",
    "from urllib import parse\n",
    "\n",
    "def loadPage(baseUrl, path, targetDir, depth, indexWriter, linkset):\n",
    "    if path in linkset :\n",
    "        return\n",
    "    \n",
    "    linkset.add(path)\n",
    "    \n",
    "    url = baseUrl + path\n",
    "    print(\"loading \", url)\n",
    "    \n",
    "    try :\n",
    "        html = urlopen(url)\n",
    "        bsObj = BeautifulSoup(html, \"html.parser\")\n",
    "        title = bsObj.find(\"h1\", {\"id\":\"firstHeading\"}).text\n",
    "\n",
    "        contentsDiv = bsObj.find(\"div\", {\"id\":\"mw-content-text\"})\n",
    "        contentsDiv = contentsDiv.find(\"div\", {\"class\":\"mw-parser-output\"})\n",
    "\n",
    "        links = []\n",
    "\n",
    "        for link in bsObj.find(\"div\", {\"id\":\"bodyContent\"}).findAll(\"a\", \n",
    "            href=re.compile(\"^(/wiki/)((?!:).)*$\")):\n",
    "              if 'href' in link.attrs:\n",
    "                  links.append(link.attrs['href'])\n",
    "\n",
    "        filename = parse.quote(title)\n",
    "        filename = filename.replace('/','_') + \".txt\"\n",
    "        filepath = targetDir + \"/\" + filename\n",
    "\n",
    "        indexWriter.writerow([title, filename])\n",
    "\n",
    "        with open(filepath, 'w', encoding='utf-8') as file :\n",
    "            file.write(title)\n",
    "            file.write(\"\\n\\n\")\n",
    "            file.write(contentsDiv.text)\n",
    "            file.close()\n",
    "\n",
    "        if(depth > 0):\n",
    "            depth -= 1\n",
    "            for link in links:\n",
    "                loadPage(baseUrl, link, targetDir, depth, indexWriter, linkset)\n",
    "    except Exception as ex :  \n",
    "        print(ex)\n",
    "    \n",
    "        \n",
    "\n",
    "# 경제사상사 : /wiki/%EA%B2%BD%EC%A0%9C%EC%82%AC%EC%83%81%EC%82%AC\n",
    "economicThoughts = \"/wiki/%EA%B2%BD%EC%A0%9C%EC%82%AC%EC%83%81%EC%82%AC\"\n",
    "# 경제학 : /wiki/%EA%B2%BD%EC%A0%9C%ED%95%99\n",
    "economics = \"/wiki/%EA%B2%BD%EC%A0%9C%ED%95%99\"\n",
    "# 은닉마르코프 모델 : /wiki/%EC%9D%80%EB%8B%89_%EB%A7%88%EB%A5%B4%EC%BD%94%ED%94%84_%EB%AA%A8%EB%8D%B8\n",
    "markov = \"/wiki/%EC%9D%80%EB%8B%89_%EB%A7%88%EB%A5%B4%EC%BD%94%ED%94%84_%EB%AA%A8%EB%8D%B8\"\n",
    "# 체계이론 : /wiki/%EB%B6%84%EB%A5%98:%EC%B2%B4%EA%B3%84%EC%9D%B4%EB%A1%A0\n",
    "system = \"/wiki/%EB%B6%84%EB%A5%98:%EC%B2%B4%EA%B3%84%EC%9D%B4%EB%A1%A0\"\n",
    "with open(\"../data/wiki_page_index.csv\", 'w', encoding='utf-8') as file :\n",
    "    indexWriter = csv.writer(file)\n",
    "    linkset = set()\n",
    "    loadPage(\"https://ko.wikipedia.org\", system, \n",
    "             \"../../data/wiki\", 2, indexWriter, linkset)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CIOBIZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import csv\n",
    "from urllib import parse\n",
    "\n",
    "host_url = \"http://www.ciobiz.co.kr\"\n",
    "article_path = \"/news/article.html?id=\"\n",
    "article_path_len = len(article_path)\n",
    "file_dir = \"../../data/ciobiz/\"\n",
    "\n",
    "\n",
    "def loadList(page, idset):\n",
    "    html = urlopen(\"http://www.ciobiz.co.kr/news/section.html?id1=01&page=\"+str(page))\n",
    "    bsObj = BeautifulSoup(html, \"html.parser\")\n",
    "    listDiv = bsObj.find(\"ul\", {\"class\":\"list_news\"})\n",
    "    for li in listDiv.findAll(\"li\"):\n",
    "        href = li.find(\"dt\").find(\"a\").attrs['href']\n",
    "        id = href[article_path_len:]\n",
    "        idset.add(id)\n",
    "\n",
    "def loadNews(url, filepath):\n",
    "    print(\"Loading \" , url)\n",
    "    html = urlopen(url)\n",
    "    bsObj = BeautifulSoup(html, \"html.parser\")\n",
    "    containerDiv = bsObj.find(\"div\", {\"id\":\"container\"})\n",
    "    article = containerDiv.find(\"article\")\n",
    "    title = article.find(\"h2\", {\"class\":\"article_title\"}).text\n",
    "    bodySect = containerDiv.find(\"section\", {\"class\":\"article_body\"})\n",
    "    body = bodySect.find(\"p\")\n",
    "    \n",
    "    with open(filepath, 'w', encoding='utf-8') as file :\n",
    "        file.write(url)\n",
    "        file.write(\"\\n\")\n",
    "        file.write(title)\n",
    "        file.write(\"\\n\")\n",
    "        file.write(body.text)\n",
    "        file.close()\n",
    "            \n",
    "idset = set()\n",
    "for page in range(5):\n",
    "    loadList(page,idset)\n",
    "\n",
    "print(len(idset))\n",
    "\n",
    "for id in idset:\n",
    "    new_url = host_url + article_path + id\n",
    "    filepath = file_dir + id + \".txt\"\n",
    "    \n",
    "    try:\n",
    "        loadNews(new_url, filepath)\n",
    "    except Exception as e:\n",
    "        print (e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ZDNet korea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen, Request\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import csv\n",
    "from urllib import parse\n",
    "\n",
    "host_url = \"http://www.zdnet.co.kr\"\n",
    "list_path = \"/news/news_keyword.asp?keyword=\" + parse.quote(\"인공지능\") + \"&page=\"\n",
    "article_path = \"/news/news_view.asp?artice_id=\"\n",
    "article_path_len = len(article_path)\n",
    "file_dir = \"../../data/zdnet/\"\n",
    "\n",
    "\n",
    "def loadList(page, idset):\n",
    "    url = host_url + list_path + str(page)\n",
    "    print(url)\n",
    "    html = urlopen(Request(url, headers={'User-Agent': 'Mozilla'}))\n",
    "    bsObj = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    for li in bsObj.findAll(\"div\", {\"class\":\"article_li\"}):\n",
    "        href = li.find(\"h2\").find(\"a\").attrs['href']\n",
    "        id = href[article_path_len:]\n",
    "        idset.add(id)\n",
    "\n",
    "def loadNews(url, filepath):\n",
    "    print(\"Loading \" , url)\n",
    "    html = urlopen(Request(url, headers={'User-Agent': 'Mozilla'}))\n",
    "    bsObj = BeautifulSoup(html, \"html.parser\")\n",
    "    \n",
    "    title = bsObj.find(\"div\", {\"class\":\"sub_view_tit2\"}).text\n",
    "    contents = bsObj.find(\"div\", {\"id\":\"content\"})\n",
    "    lines = contents.findAll(\"p\")\n",
    "    \n",
    "    with open(filepath, 'w', encoding='utf-8') as file :\n",
    "        try:\n",
    "            file.write(url)\n",
    "            file.write(\"\\n\")\n",
    "            file.write(title)\n",
    "            for line in lines:\n",
    "                file.write(\"\\n\")\n",
    "                file.write(line.text)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "        file.close()\n",
    "            \n",
    "idset = set()\n",
    "for page in range(10):\n",
    "    loadList(page+1,idset)\n",
    "\n",
    "print(len(idset))\n",
    "\n",
    "for id in idset:\n",
    "    new_url = host_url + article_path + id\n",
    "    filepath = file_dir + id + \".txt\"\n",
    "    \n",
    "    try:\n",
    "        loadNews(new_url, filepath)\n",
    "    except Exception as e:\n",
    "        print (e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Blotter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen, Request\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import csv\n",
    "from urllib import parse\n",
    "\n",
    "host_url = \"http://www.bloter.net\"\n",
    "list_path = \"/archives/category/society/page/\"\n",
    "article_path = \"/archives/\"\n",
    "article_path_len = len(\"article-\")\n",
    "file_dir = \"../../data/blotter/\"\n",
    "\n",
    "\n",
    "def loadList(page, idset):\n",
    "    url = host_url + list_path + str(page)\n",
    "    print(url)\n",
    "    html = urlopen(Request(url, headers={'User-Agent': 'Mozilla'}))\n",
    "    bsObj = BeautifulSoup(html, \"html.parser\")\n",
    "    body = bsObj.find(\"div\", {\"class\":\"category--body--main\"})\n",
    "\n",
    "    for li in body.findAll(\"article\"):\n",
    "        nodeid = li.attrs['id']\n",
    "        id = nodeid[article_path_len:]\n",
    "        idset.add(id)\n",
    "\n",
    "def loadNews(url, filepath):\n",
    "    print(\"Loading \" , url)\n",
    "    html = urlopen(Request(url, headers={'User-Agent': 'Mozilla'}))\n",
    "    bsObj = BeautifulSoup(html, \"html.parser\")\n",
    "    \n",
    "    title = bsObj.find(\"h1\", {\"class\":\"headline\"}).text\n",
    "    contents = bsObj.find(\"div\", {\"class\":\"article--content\"})\n",
    "    lines = contents.findAll(\"p\")\n",
    "    \n",
    "    with open(filepath, 'w', encoding='utf-8') as file :\n",
    "        try:\n",
    "            file.write(url)\n",
    "            file.write(\"\\n\")\n",
    "            file.write(title)\n",
    "            for line in lines:\n",
    "                file.write(\"\\n\")\n",
    "                file.write(line.text)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "        file.close()\n",
    "            \n",
    "idset = set()\n",
    "for page in range(10):\n",
    "    loadList(page+1,idset)\n",
    "\n",
    "print(len(idset))\n",
    "\n",
    "for id in idset:\n",
    "    new_url = host_url + article_path + id\n",
    "    filepath = file_dir + id + \".txt\"\n",
    "    \n",
    "    try:\n",
    "        loadNews(new_url, filepath)\n",
    "    except Exception as e:\n",
    "        print (e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체기사 :  뉴스 : 한겨레  뉴스 - 인터넷한겨레\n",
      "..............................중앙일보 | 전체기사\n",
      "..............................경향신문:전체기사\n",
      ".....................................................................................http://news.khan.co.kr/kh_news/khan_art_view.html?artid=201808241518001&code=910303&utm_campaign=rss_btn_click&utm_source=khan_rss&utm_medium=rss&utm_content=total_news 'NoneType' object has no attribute 'text'\n",
      "............................................................http://news.khan.co.kr/kh_news/khan_art_view.html?artid=201808240000001&code=940100&utm_campaign=rss_btn_click&utm_source=khan_rss&utm_medium=rss&utm_content=total_news 'NoneType' object has no attribute 'text'\n",
      "..............................................................................................................................................................................http://news.khan.co.kr/kh_news/khan_art_view.html?artid=201808231106001&code=940100&utm_campaign=rss_btn_click&utm_source=khan_rss&utm_medium=rss&utm_content=total_news 'NoneType' object has no attribute 'text'\n",
      "........................................................http://news.khan.co.kr/kh_news/khan_art_view.html?artid=201808230010001&code=940100&utm_campaign=rss_btn_click&utm_source=khan_rss&utm_medium=rss&utm_content=total_news 'NoneType' object has no attribute 'text'\n",
      ".노컷 뉴스\n",
      "..http://www.nocutnews.co.kr/show.asp?idx=4268018 'NoneType' object has no attribute 'text'\n",
      "..http://www.nocutnews.co.kr/show.asp?idx=4268036 'NoneType' object has no attribute 'text'\n",
      "............"
     ]
    }
   ],
   "source": [
    "#!pip install feedparser\n",
    "\n",
    "from urllib.request import urlopen, Request\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import os\n",
    "import csv\n",
    "from urllib import parse\n",
    "\n",
    "import feedparser\n",
    "\n",
    "def handleKhan(bsObj):\n",
    "    ttlObj = bsObj.find(\"h1\",{\"id\":\"article_title\"})\n",
    "    title = ttlObj.text if ttlObj else ''\n",
    "    contents = bsObj.find(\"div\",{\"class\":\"art_body\"}).text\n",
    "    return title, contents\n",
    "\n",
    "\n",
    "def handleJoins(bsObj):\n",
    "    title = bsObj.find(\"h1\",{\"id\":\"article_title\"}).text\n",
    "    contents = bsObj.find(\"div\",{\"id\":\"article_body\"}).text\n",
    "    return title, contents\n",
    "\n",
    "\n",
    "def handleHani(bsObj):\n",
    "    title = bsObj.find(\"span\",{\"class\":\"title\"}).text\n",
    "    contents = bsObj.find(\"div\",{\"class\":\"article-text\"}).find(\"div\",{\"class\":\"text\"}).text\n",
    "    return title, contents\n",
    "\n",
    "\n",
    "def handleNocut(bsObj):\n",
    "    title = bsObj.find(\"div\", {\"id\":\"pnlViewTop\"}).find(\"h2\").text\n",
    "    contents = bsObj.find(\"div\",{\"id\":\"pnlViewBox\"}).text\n",
    "    return title, contents\n",
    "\n",
    "\n",
    "rssfeed_map = {'hani':'http://www.hani.co.kr/rss/',\n",
    "                'joins':'https://rss.joins.com/joins_news_list.xml',\n",
    "               'khan':'http://www.khan.co.kr/rss/rssdata/total_news.xml', \n",
    "                'nocut':'http://rss.nocutnews.co.kr/nocutnews.xml'}\n",
    "\n",
    "handler_map = {'hani':handleHani,\n",
    "                'joins':handleJoins,\n",
    "               'khan':handleKhan,\n",
    "                'nocut':handleNocut}\n",
    "\n",
    "newsdir = \"../../data/news\"\n",
    "\n",
    "\n",
    "def loadpage(url, filepath, dochandler):\n",
    "    try :\n",
    "        html = urlopen(Request(url, headers={'User-Agent': 'Mozilla'}))\n",
    "        bsObj = BeautifulSoup(html, \"html.parser\")\n",
    "        title, contents = dochandler(bsObj)\n",
    "\n",
    "        with open(filepath, 'w', encoding='utf-8') as file :\n",
    "            file.write(url)\n",
    "            file.write(\"\\n\")\n",
    "            file.write(title)\n",
    "            file.write(\"\\n\")\n",
    "            file.write(contents)\n",
    "            file.close()\n",
    "    except Exception as ex :  \n",
    "        print(url,ex)\n",
    "    \n",
    "for name, url in rssfeed_map.items():\n",
    "    feed = feedparser.parse(url)\n",
    "    print(feed['feed']['title'])\n",
    "    length = len(feed['entries'])\n",
    "    \n",
    "    handler = handler_map[name]\n",
    "    \n",
    "    for index in range(length):\n",
    "        newsurl = feed.entries[index]['link']\n",
    "#         print(newsurl)\n",
    "        filepath = os.path.join(newsdir,name+\"-\"+str(index) + \".txt\")\n",
    "        loadpage(newsurl, filepath, handler)\n",
    "        print('.', end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_conda)",
   "language": "python",
   "name": "conda_conda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
