{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Korean Text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init Trainer\n",
      "Init DocResolver\n",
      "3239 files\n",
      "Loading Dictionary from ../samples/dic/dictionary.csv\n",
      "Load  ../samples/dic/dictionary.csv   소요시간: 0.003\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import csv\n",
    "import os\n",
    "from pathlib import Path\n",
    "import sys\n",
    "sys.path.insert(0, '../')\n",
    "import pycor.korlang as korlang\n",
    "import pycor.utils as utils\n",
    "from pycor.res import WikiResolver\n",
    "\n",
    "stopwatch = utils.StopWatch()\n",
    "\n",
    "docsize = 7000\n",
    "\n",
    "#for aux in lm.auxmap.values():\n",
    "#    print(aux)\n",
    "\n",
    "def listfiles(path):\n",
    "    result_arr = []\n",
    "    filenames = os.listdir(path)\n",
    "    for filename in filenames:\n",
    "        full_filename = os.path.join(path, filename)\n",
    "        result_arr.append(full_filename)\n",
    "    return result_arr\n",
    "    \n",
    "files = listfiles('../samples')\n",
    "files = sorted(files)\n",
    "files.extend( listfiles('../../data/wiki2') )\n",
    "files.extend( listfiles('../../data/wiki') )\n",
    "files.extend( listfiles('../../data/NP') )\n",
    "\n",
    "print (len(files), \"files\")\n",
    "\n",
    "outputpath = \"../../output/\" + str(docsize) + \"/\"\n",
    "model_path = outputpath + \"model/\"\n",
    "\n",
    "os.makedirs(outputpath, exist_ok=True)\n",
    "os.makedirs(model_path, exist_ok=True)\n",
    "\n",
    "\n",
    "dictionary_path = \"../samples/dic/dictionary.csv\"\n",
    "\n",
    "trainer = korlang._trainer\n",
    "\n",
    "# 사전 등록 \n",
    "trainer.loaddic(dictionary_path)\n",
    "# wikiResolver = WikiResolver(trainer.wordmap)\n",
    "# trainer.setresolver(wikiResolver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Texts >>> \n",
      "100>200>300>400>500>600>700>800>900>1000>\n",
      "1100>1200>1300>1400>1500>1600>1700>1800>1900>2000>\n",
      "2100>2200>2300>2400>2500>2600>2700>2800>2900>3000>\n",
      "3100>3200>Load Texts:  543s.990ms. ( 543,990 ms.)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "stopwatch.start()\n",
    "\n",
    "def extractHeads(words) :\n",
    "    word_texts = []\n",
    "    for word in words:\n",
    "        if word.bestpair:\n",
    "            word_texts.append(word.bestpair.head.text)\n",
    "        else:\n",
    "            word_texts.append(word.text)\n",
    "    return word_texts\n",
    "\n",
    "\n",
    "def extractTails(words) :\n",
    "    word_texts = []\n",
    "    for word in words:\n",
    "        if word.bestpair:\n",
    "            #word_texts.append(word.bestpair.tail.text + \":\" + str(word.bestpair.type))\n",
    "            word_texts.append(word.bestpair.tail.text)\n",
    "        else:\n",
    "            word_texts.append(\"\")\n",
    "            #word_texts.append(word.text)\n",
    "    return word_texts\n",
    "\n",
    "\n",
    "\n",
    "def extractTags(words) :\n",
    "    word_texts = []\n",
    "    for word in words:\n",
    "        if word.bestpair:\n",
    "            word_texts.append(word.bestpair.tags)\n",
    "        else:\n",
    "            word_texts.append(\"*\")\n",
    "    return word_texts\n",
    "\n",
    "\n",
    "\n",
    "def writeTails(outputpath, index, sentence_array):\n",
    "    outputfile = outputpath + 'samples/' + str(index) + '_tails.txt'\n",
    "    \n",
    "    words_array = trainer.scoreDocument(sentence_array)\n",
    "    \n",
    "    with open(outputfile, 'w', encoding='utf-8') as out:\n",
    "        writer = csv.writer(out)\n",
    "        for words in words_array:\n",
    "            word_texts = extractTails(words)\n",
    "            writer.writerow(word_texts)\n",
    "        out.close()\n",
    "        \n",
    "def writeAll(tailswriter, headswriter, tagswriter, sentence_array):\n",
    "    words_array = trainer.scoreDocument(sentence_array)\n",
    "    \n",
    "    for words in words_array:\n",
    "        word_texts = extractTails(words)\n",
    "        \n",
    "        if len(word_texts) > 2:\n",
    "            tailswriter.writerow(word_texts)\n",
    "            heads = extractHeads(words)\n",
    "            headswriter.writerow(heads)\n",
    "            tags = extractTags(words)\n",
    "            tagswriter.writerow(tags)\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "tailsfile = open(outputpath + 'tails.txt', 'w', encoding='utf-8')\n",
    "tailswriter = csv.writer(tailsfile)\n",
    "\n",
    "\n",
    "headsfile = open(outputpath + 'heads.txt', 'w', encoding='utf-8')\n",
    "headswriter = csv.writer(headsfile)\n",
    "\n",
    "\n",
    "tagsfile = open(outputpath + 'tags.txt', 'w', encoding='utf-8')\n",
    "tagswriter = csv.writer(tagsfile)\n",
    "\n",
    "print(\"Loading Texts >>> \")\n",
    "index =0\n",
    "for file in files[:docsize]:\n",
    "    if file.endswith(\".txt\") :\n",
    "        sentence_array, words_array = trainer.loadfile(file)\n",
    "        writeAll(tailswriter,headswriter, tagswriter,sentence_array)\n",
    "        index += 1\n",
    "        if index % 100 == 0:\n",
    "            print(index,end=\">\")\n",
    "            if index % 1000 == 0:\n",
    "                print()\n",
    "            \n",
    "tailsfile.close()\n",
    "tagsfile.close()\n",
    "\n",
    "print(\"Load Texts: \" , stopwatch.secmilli() , \"(\", stopwatch.millisecstr(), \"ms.)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write  ../../output/7000/np_words.csv   소요시간: 6s.055ms. ( 6,054 ms.)\n",
      "Write  ../../output/7000/np_words_scored.csv   소요시간: 3s.507ms. ( 3,507 ms.)\n"
     ]
    }
   ],
   "source": [
    "def writeWord(writer, word):\n",
    "    if len(word.particles) == 0:\n",
    "        writer.writerow([word.text, 'X'])\n",
    "    for part in word.particles:\n",
    "        h = part.head.text if part.head else ''\n",
    "        t = part.tail.text if part.tail else ''\n",
    "        writer.writerow([word.text, h, t, part.score, part.tags, part.pos])\n",
    "\n",
    "utils.writecsv(outputpath + \"np_words.csv\", trainer.wordmap.words.values(), writeWord)\n",
    "\n",
    "def writeWord(writer, word):\n",
    "    bestpair = word.bestpair\n",
    "    if bestpair :\n",
    "        h = bestpair.head.text if bestpair.head else ''\n",
    "        t = bestpair.tail.text if bestpair.tail else 'X'\n",
    "        writer.writerow([ word.text, '   ', h, t, '   ', bestpair.score, bestpair.tags, bestpair.pos ])\n",
    "    else:\n",
    "        writer.writerow([ word.text, 'X' ])\n",
    "        \n",
    "utils.writecsv(outputpath + \"np_words_scored.csv\", trainer.wordmap.words.values(), writeWord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build Vocab:  0s.000ms. ( 0 ms.)\n",
      "Word Count :  660,119\n",
      "head Count :  470,794\n",
      "tail Count :  1,182\n",
      "Single Count: 204676\n",
      "용언 Count: 25636\n",
      "체언 Count: 105617\n",
      "Ambiguous Count: 11936\n",
      "Heads Count: 470794\n",
      "Tails Count: 1182\n",
      "Clear words size: 660119\n",
      "Saving Model to ../../output/7000/model/\n",
      "Save  ../../output/7000/model//heads.csv   소요시간: 1.705\n",
      "Save  ../../output/7000/model//tails.csv   소요시간: 0.01\n",
      "Clear Heads: 321200 remains.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "stopwatch.start()\n",
    "\n",
    "print(\"Build Vocab: \" , stopwatch.secmilli() , \"(\", stopwatch.millisecstr(), \"ms.)\")\n",
    "print(\"Word Count : \" , utils.comma(len(trainer.wordmap.words)) )\n",
    "print(\"head Count : \" , utils.comma(len(trainer.wordmap.heads)) )\n",
    "print(\"tail Count : \" , utils.comma(len(trainer.wordmap.tails)) )\n",
    "\n",
    "\n",
    "\n",
    "snglist, ylist, clist, ambilist = trainer.buildVocab()\n",
    "\n",
    "\n",
    "trainer.savemodel(model_path)\n",
    "\n",
    "# with open(model_path + \"ambiguous.csv\", 'w', encoding='utf-8') as csvfile :\n",
    "#     writer = csv.writer(csvfile)\n",
    "#     for head in ambilist:\n",
    "#         writer.writerow([head.text, '+'.join(head.pos), head.tails])\n",
    "            \n",
    "#     csvfile.close()\n",
    "\n",
    "# with open(model_path + \"single.csv\", 'w', encoding='utf-8') as csvfile :\n",
    "#     writer = csv.writer(csvfile)\n",
    "#     for head in snglist:\n",
    "#         writer.writerow([head.text, '+'.join(head.pos), head.tails])\n",
    "            \n",
    "#     csvfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write  ../../output/7000/np_tails.csv   소요시간: 0s.008ms. ( 7 ms.)\n",
      "Write  ../../output/7000/np_tails_occ.csv   소요시간: 0s.010ms. ( 9 ms.)\n"
     ]
    }
   ],
   "source": [
    "def writeTails(writer, tail):\n",
    "    #row = [tail.text]\n",
    "    #for head in tail.heads:\n",
    "    #    row.append(head.text)\n",
    "    #row = [tail.text, len(tail.heads), tail.heads]\n",
    "    row = [tail.text, len(tail.heads)]\n",
    "    writer.writerow(row)\n",
    "\n",
    "tailsorted = sorted(trainer.wordmap.tails.values(), key=lambda tail:tail.text[len(tail.text)-1])\n",
    "utils.writecsv(outputpath + \"np_tails.csv\", tailsorted, writeTails)\n",
    "\n",
    "\n",
    "tailsorted2 = sorted(trainer.wordmap.tails.values(), key=lambda tail:len(tail.heads), reverse=True)\n",
    "utils.writecsv(outputpath + \"np_tails_occ.csv\", tailsorted2, writeTails)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write  ../../output/7000/np_heads.csv   소요시간: 0s.862ms. ( 861 ms.)\n",
      "Write  ../../output/7000/np_heads_occ.csv   소요시간: 0s.804ms. ( 803 ms.)\n",
      "Write  ../../output/7000/np_head_scored.csv   소요시간: 1s.144ms. ( 1,143 ms.)\n"
     ]
    }
   ],
   "source": [
    "def writeHeads(writer, head):\n",
    "    row = [ str(head), '  ', len(head.tails), ' ']\n",
    "    for tail in head.tails:\n",
    "        row.append(tail.text)\n",
    "    writer.writerow(row)\n",
    "\n",
    "headsorted = sorted(trainer.wordmap.heads.values(), key=lambda head:head.text)\n",
    "utils.writecsv(outputpath + \"np_heads.csv\", headsorted, writeHeads)\n",
    "\n",
    "\n",
    "headsorted2 = sorted(trainer.wordmap.heads.values(), key=lambda head:len(head.tails), reverse=True)\n",
    "utils.writecsv(outputpath + \"np_heads_occ.csv\", headsorted2, writeHeads)\n",
    "\n",
    "\n",
    "headset = set()\n",
    "\n",
    "for word in trainer.wordmap.words.values():\n",
    "    bestpair = word.bestpair\n",
    "    if bestpair and bestpair.head:\n",
    "        headset.add(bestpair.head)\n",
    "        \n",
    "def writeHead(writer, head):\n",
    "    row = [ str(head), '\\t', len(head.tails), '\\t']\n",
    "    \n",
    "    for tail in head.tails:\n",
    "        row.append(tail.text)\n",
    "        \n",
    "    writer.writerow(row)\n",
    "        \n",
    "\n",
    "        \n",
    "headlist = sorted(headset , key=lambda head:head.text )\n",
    "\n",
    "utils.writecsv(outputpath + \"np_head_scored.csv\", headlist, writeHead)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Model from ../../output/7000/model/\n",
      "Load  ../../output/7000/model//heads.csv   소요시간: 1.086\n",
      "Load  ../../output/7000/model//tails.csv   소요시간: 0.013\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def extractWords(words) :\n",
    "    word_texts = []\n",
    "    for word in words:\n",
    "        if word.bestpair:\n",
    "            #word_texts.append(word.bestpair.head.text +\":\"+word.bestpair.tail.text)\n",
    "            pos = word.bestpair.pos if word.bestpair.pos else ''\n",
    "            tag = word.bestpair.tags if word.bestpair.tags else ''\n",
    "#             print(word.bestpair.tail.occurence())\n",
    "            postag = None\n",
    "            if pos or tag:\n",
    "                postag = \"(\"+ str(pos) + \":\" + str(tag) +\")\"\n",
    "            else :\n",
    "                postag = \"\"\n",
    "            word_texts.append(word.bestpair.head.text + postag)\n",
    "        else:\n",
    "            word_texts.append(word.text)\n",
    "    return word_texts\n",
    "\n",
    "def writeExtracted(outputpath, suffix, index, sentence_array):\n",
    "    outputfile = outputpath + str(index) + suffix + '.txt'\n",
    "    \n",
    "    words_array = trainer.scoreDocument(sentence_array)\n",
    "\n",
    "    with open(outputfile, 'w', encoding='utf-8') as out:\n",
    "        writer = csv.writer(out)\n",
    "        for words in words_array:\n",
    "            word_texts = extractWords(words)\n",
    "            writer.writerow(word_texts)\n",
    "        out.close()\n",
    "        \n",
    "index = 0\n",
    "for file in files[:5]:\n",
    "    if file.endswith(\".txt\") :\n",
    "        sentence_array, words_array = trainer.loadfile(file)\n",
    "        index += 1\n",
    "        writeExtracted(outputpath, '_train', index, sentence_array)\n",
    "\n",
    "import pycor\n",
    "pycor.loadmodel(model_path)\n",
    "\n",
    "index = 0\n",
    "for file in files[:5]:\n",
    "    if file.endswith(\".txt\") :\n",
    "        sentence_array, words_array = pycor.readfile(file)\n",
    "        index += 1\n",
    "        writeExtracted(outputpath, '_pycor', index, sentence_array)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_conda)",
   "language": "python",
   "name": "conda_conda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
