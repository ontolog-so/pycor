{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Korean Text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init Trainer\n",
      "Init DocResolver\n",
      "3240 files\n",
      "Loading Dictionary from ../samples/dic/dictionary.csv\n",
      "Load Dic ../samples/dic/dictionary.csv   소요시간: 0.004\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import csv\n",
    "import os\n",
    "from pathlib import Path\n",
    "import sys\n",
    "sys.path.insert(0, '../')\n",
    "import pycor\n",
    "# import pycor.korlang as korlang\n",
    "import pycor.utils as utils\n",
    "import pycor.scoring as scoring\n",
    "from pycor.res import WikiResolver\n",
    "from pycor.res import CollocationResolver\n",
    "from pycor.res import yongeonresolver as yr\n",
    "\n",
    "stopwatch = utils.StopWatch()\n",
    "\n",
    "docsize = 3000\n",
    "\n",
    "#for aux in lm.auxmap.values():\n",
    "#    print(aux)\n",
    "\n",
    "def listfiles(path):\n",
    "    result_arr = []\n",
    "    filenames = os.listdir(path)\n",
    "    for filename in filenames:\n",
    "        full_filename = os.path.join(path, filename)\n",
    "        result_arr.append(full_filename)\n",
    "    return result_arr\n",
    "    \n",
    "files = listfiles('../samples')\n",
    "files = sorted(files)\n",
    "files.extend( listfiles('../../data/wiki2') )\n",
    "files.extend( listfiles('../../data/wiki') )\n",
    "files.extend( listfiles('../../data/NP') )\n",
    "\n",
    "print (len(files), \"files\")\n",
    "\n",
    "outputpath = \"../../output/\" + str(docsize) + \"/\"\n",
    "model_path = outputpath + \"model/\"\n",
    "\n",
    "os.makedirs(outputpath, exist_ok=True)\n",
    "os.makedirs(model_path, exist_ok=True)\n",
    "\n",
    "\n",
    "dictionary_path = \"../samples/dic/dictionary.csv\"\n",
    "\n",
    "# trainer = korlang._trainer\n",
    "\n",
    "# 사전 등록 \n",
    "pycor.loaddic(dictionary_path)\n",
    "# wikiResolver = WikiResolver()\n",
    "# pycor.addresolver(wikiResolver)\n",
    "\n",
    "collocationResolver = CollocationResolver()\n",
    "pycor.addresolver(collocationResolver)\n",
    "\n",
    "yongeonResolver = yr.YongeonResolver()\n",
    "pycor.addresolver(yongeonResolver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Texts >>> \n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'Sentence' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-ce3603c84800>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".txt\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0mwords_array\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpycor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m         \u001b[0mwriteAll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtailswriter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mheadswriter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtagswriter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mwords_array\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-ce3603c84800>\u001b[0m in \u001b[0;36mwriteAll\u001b[0;34m(tailswriter, headswriter, tagswriter, words_array)\u001b[0m\n\u001b[1;32m     58\u001b[0m             \u001b[0mheads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextractHeads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m             \u001b[0mheadswriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriterow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m             \u001b[0mtags\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextractTags\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m             \u001b[0mtagswriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriterow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-ce3603c84800>\u001b[0m in \u001b[0;36mextractTags\u001b[0;34m(words)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mextractTags\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mword_texts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbestpair\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0mword_texts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbestpair\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'Sentence' object is not iterable"
     ]
    }
   ],
   "source": [
    "\n",
    "stopwatch.start()\n",
    "\n",
    "def extractHeads(words) :\n",
    "    word_texts = []\n",
    "    for word in words.words:\n",
    "        if word.bestpair:\n",
    "            word_texts.append(word.bestpair.head.text)\n",
    "        else:\n",
    "            word_texts.append(word.text)\n",
    "    return word_texts\n",
    "\n",
    "\n",
    "def extractTails(words) :\n",
    "    word_texts = []\n",
    "    for word in words.words:\n",
    "        if word.bestpair:\n",
    "            #word_texts.append(word.bestpair.tail.text + \":\" + str(word.bestpair.type))\n",
    "            word_texts.append(word.bestpair.tail.text)\n",
    "        else:\n",
    "            word_texts.append(\"\")\n",
    "            #word_texts.append(word.text)\n",
    "    return word_texts\n",
    "\n",
    "\n",
    "\n",
    "def extractTags(words) :\n",
    "    word_texts = []\n",
    "    for word in words.words:\n",
    "        if word.bestpair:\n",
    "            word_texts.append(word.bestpair.tags)\n",
    "        else:\n",
    "            word_texts.append(\"*\")\n",
    "    return word_texts\n",
    "\n",
    "\n",
    "\n",
    "def writeTails(outputpath, index, words_array):\n",
    "    outputfile = outputpath + 'samples/' + str(index) + '_tails.txt'\n",
    "    \n",
    "#     words_array = trainer.scoreDocument(sentence_array)\n",
    "    \n",
    "    with open(outputfile, 'w', encoding='utf-8') as out:\n",
    "        writer = csv.writer(out)\n",
    "        for words in words_array:\n",
    "            word_texts = extractTails(words)\n",
    "            writer.writerow(word_texts)\n",
    "        out.close()\n",
    "        \n",
    "def writeAll(tailswriter, headswriter, tagswriter, words_array):\n",
    "#     words_array = trainer.scoreDocument(sentence_array)\n",
    "    \n",
    "    for words in words_array:\n",
    "        word_texts = extractTails(words)\n",
    "        \n",
    "        if len(word_texts) > 2:\n",
    "            tailswriter.writerow(word_texts)\n",
    "            heads = extractHeads(words)\n",
    "            headswriter.writerow(heads)\n",
    "            tags = extractTags(words)\n",
    "            tagswriter.writerow(tags)\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "tailsfile = open(outputpath + 'tails.txt', 'w', encoding='utf-8')\n",
    "tailswriter = csv.writer(tailsfile)\n",
    "\n",
    "\n",
    "headsfile = open(outputpath + 'heads.txt', 'w', encoding='utf-8')\n",
    "headswriter = csv.writer(headsfile)\n",
    "\n",
    "\n",
    "tagsfile = open(outputpath + 'tags.txt', 'w', encoding='utf-8')\n",
    "tagswriter = csv.writer(tagsfile)\n",
    "\n",
    "print(\"Loading Texts >>> \")\n",
    "index =0\n",
    "for file in files[:docsize]:\n",
    "    if file.endswith(\".txt\") :\n",
    "        words_array = pycor.readfile(file)\n",
    "        writeAll(tailswriter,headswriter, tagswriter,words_array)\n",
    "        index += 1\n",
    "        if index % 100 == 0:\n",
    "            print(index,end=\">\")\n",
    "            if index % 1000 == 0:\n",
    "                print()\n",
    "            \n",
    "tailsfile.close()\n",
    "tagsfile.close()\n",
    "\n",
    "print(\"Load Texts: \" , stopwatch.secmilli() , \"(\", stopwatch.millisecstr(), \"ms.)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def writeWord(writer, word):\n",
    "    if len(word.particles) == 0:\n",
    "        writer.writerow([word.text, 'X'])\n",
    "    for part in word.particles:\n",
    "        h = part.head.text if part.head else ''\n",
    "        t = part.tail.text if part.tail else ''\n",
    "        writer.writerow([word.text, h, t, part.score, part.tags, part.pos])\n",
    "\n",
    "utils.writecsv(outputpath + \"np_words.csv\", pycor.getmodel().words.values(), writeWord)\n",
    "\n",
    "def writeWord(writer, word):\n",
    "    bestpair = word.bestpair\n",
    "    if bestpair :\n",
    "        h = bestpair.head.text if bestpair.head else ''\n",
    "        t = bestpair.tail.text if bestpair.tail else 'X'\n",
    "        writer.writerow([ word.text, '   ', h, t, '   ', bestpair.score, bestpair.tags, bestpair.pos ])\n",
    "    else:\n",
    "        writer.writerow([ word.text, 'X' ])\n",
    "        \n",
    "utils.writecsv(outputpath + \"np_words_scored.csv\", pycor.getmodel().words.values(), writeWord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pycor.dev.model as dev\n",
    "\n",
    "stopwatch.start()\n",
    "\n",
    "snglist, ylist, clist, ambilist = pycor.buildvocab()\n",
    "\n",
    "wordmap = pycor.getmodel()\n",
    "\n",
    "print(\"Build Vocab: \" , stopwatch.secmilli() , \"(\", stopwatch.millisecstr(), \"ms.)\")\n",
    "print(\"Word Count : \" , utils.comma(len(wordmap.words)) )\n",
    "print(\"head Count : \" , utils.comma(len(wordmap.heads)) )\n",
    "print(\"tail Count : \" , utils.comma(len(wordmap.tails)) )\n",
    "\n",
    "oldmodel = None\n",
    "\n",
    "try:\n",
    "    oldmodel = dev.loadmodel(model_path)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "if oldmodel:\n",
    "    print(\"Old Heads:\", len(oldmodel.heads))\n",
    "    result = dev.compareHeads(oldmodel.heads, wordmap.heads)\n",
    "    dev.saveCompHeads(model_path+\"/compare.csv\", result)\n",
    "\n",
    "pycor.savemodel(model_path)\n",
    "\n",
    "\n",
    "# with open(model_path + \"ambiguous.csv\", 'w', encoding='utf-8') as csvfile :\n",
    "#     writer = csv.writer(csvfile)\n",
    "#     for head in ambilist:\n",
    "#         writer.writerow([head.text, '+'.join(head.pos), head.tails])\n",
    "            \n",
    "#     csvfile.close()\n",
    "\n",
    "# with open(model_path + \"single.csv\", 'w', encoding='utf-8') as csvfile :\n",
    "#     writer = csv.writer(csvfile)\n",
    "#     for head in snglist:\n",
    "#         writer.writerow([head.text, '+'.join(head.pos), head.tails])\n",
    "            \n",
    "#     csvfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "yongeonResolver.writemap(outputpath + \"/yongeon.csv\")\n",
    "yongeonResolver.writeindex(outputpath + \"/yongeon_index.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def writeTails(writer, tail):\n",
    "    #row = [tail.text]\n",
    "    #for head in tail.heads:\n",
    "    #    row.append(head.text)\n",
    "    #row = [tail.text, len(tail.heads), tail.heads]\n",
    "    row = [tail.text, len(tail.heads)]\n",
    "    writer.writerow(row)\n",
    "\n",
    "wordmap = pycor.getmodel()\n",
    "tailsorted = sorted(wordmap.tails.values(), key=lambda tail:tail.text[len(tail.text)-1])\n",
    "utils.writecsv(outputpath + \"np_tails.csv\", tailsorted, writeTails)\n",
    "\n",
    "\n",
    "tailsorted2 = sorted(wordmap.tails.values(), key=lambda tail:len(tail.heads), reverse=True)\n",
    "utils.writecsv(outputpath + \"np_tails_occ.csv\", tailsorted2, writeTails)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def writeHeads(writer, head):\n",
    "    row = [ str(head), '  ',head.score, ' ', len(head.tails), ' ']\n",
    "    for tail in head.tails:\n",
    "        row.append(tail.text)\n",
    "    writer.writerow(row)\n",
    "\n",
    "wordmap = pycor.getmodel()\n",
    "\n",
    "headsorted = sorted(wordmap.heads.values(), key=lambda head:head.text)\n",
    "utils.writecsv(outputpath + \"np_heads.csv\", headsorted, writeHeads)\n",
    "\n",
    "\n",
    "headsorted2 = sorted( wordmap.heads.values(), key=lambda head:len(head.tails), reverse=True)\n",
    "utils.writecsv(outputpath + \"np_heads_occ.csv\", headsorted2, writeHeads)\n",
    "\n",
    "\n",
    "headset = set()\n",
    "\n",
    "for word in  wordmap.words.values():\n",
    "    bestpair = word.bestpair\n",
    "    if bestpair and bestpair.head:\n",
    "        headset.add(bestpair.head)\n",
    "        \n",
    "def writeHead(writer, head):\n",
    "    row = [ str(head), '\\t', len(head.tails), '\\t']\n",
    "    \n",
    "    for tail in head.tails:\n",
    "        row.append(tail.text)\n",
    "        \n",
    "    writer.writerow(row)\n",
    "        \n",
    "\n",
    "        \n",
    "headlist = sorted(headset , key=lambda head:head.text )\n",
    "\n",
    "utils.writecsv(outputpath + \"np_head_scored.csv\", headlist, writeHead)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extractWords(words) :\n",
    "    word_texts = []\n",
    "    for word in words:\n",
    "        if word.bestpair:\n",
    "            #word_texts.append(word.bestpair.head.text +\":\"+word.bestpair.tail.text)\n",
    "            pos = word.bestpair.pos if word.bestpair.pos else ''\n",
    "            tag = word.bestpair.tags if word.bestpair.tags else ''\n",
    "#             print(word.bestpair.tail.occurence())\n",
    "            postag = None\n",
    "            if pos or tag:\n",
    "                postag = \"(\"+ str(pos) + \":\" + str(tag) +\")\"\n",
    "            else :\n",
    "                postag = \"\"\n",
    "            word_texts.append(word.bestpair.head.text + postag)\n",
    "        else:\n",
    "            word_texts.append(word.text)\n",
    "    return word_texts\n",
    "\n",
    "def writeExtracted(outputpath, suffix, index, words_array):\n",
    "    outputfile = outputpath + str(index) + suffix + '.txt'\n",
    "    \n",
    "#     words_array = trainer.scoreDocument(sentence_array)\n",
    "\n",
    "    with open(outputfile, 'w', encoding='utf-8') as out:\n",
    "        writer = csv.writer(out)\n",
    "        for words in words_array:\n",
    "            word_texts = extractWords(words)\n",
    "            writer.writerow(word_texts)\n",
    "        out.close()\n",
    "        \n",
    "index = 0\n",
    "for file in files[:5]:\n",
    "    if file.endswith(\".txt\") :\n",
    "        sentence_array, words_array = pycor.readfile(file)\n",
    "        index += 1\n",
    "        writeExtracted(outputpath, '_train', index, words_array)\n",
    "\n",
    "import pycor\n",
    "pycor.loadmodel(model_path)\n",
    "\n",
    "index = 0\n",
    "for file in files[:5]:\n",
    "    if file.endswith(\".txt\") :\n",
    "        sentence_array, words_array = pycor.readfile(file)\n",
    "        index += 1\n",
    "        writeExtracted(outputpath, '_pycor', index, words_array)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word = pycor.resolveword('라부아지에', debug=True)\n",
    "print(word.bestpair)\n",
    "word = pycor.resolveword('라부아지에에게')\n",
    "print(word.bestpair)\n",
    "word = pycor.resolveword('라부아지에가', debug=True)\n",
    "print(word.bestpair)\n",
    "word = pycor.resolveword('라부아지에를')\n",
    "print(word.bestpair)\n",
    "word = pycor.resolveword('라부아지에', debug=True)\n",
    "print(word.bestpair)\n",
    "\n",
    "word = pycor.resolveword('간디에')\n",
    "print(word.bestpair)\n",
    "word = pycor.resolveword('간디에게')\n",
    "print(word.bestpair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word = pycor.resolveword('감소되서')\n",
    "print(word.bestpair)\n",
    "word = pycor.resolveword('감소돼서')\n",
    "print(word.bestpair)\n",
    "word = pycor.resolveword('감소되었다가')\n",
    "print(word.bestpair)\n",
    "word = pycor.resolveword('감소됐다가', debug=True)\n",
    "print(word.bestpair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_conda)",
   "language": "python",
   "name": "conda_conda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
