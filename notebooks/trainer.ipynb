{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Korean Text\n",
    "* from korean wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2794 files\n",
      "Init SentenceParser\n",
      "Init Trainer\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import csv\n",
    "import os\n",
    "from pathlib import Path\n",
    "import sys\n",
    "sys.path.insert(0, '../')\n",
    "import pycor.utils as utils\n",
    "import pycor.parser as ps\n",
    "import pycor.trainer as tr\n",
    "import pycor.langmodel as lm\n",
    "import pycor.speechmodel as sm\n",
    "import pycor.korutils as ku\n",
    "import pycor.std.korstandard\n",
    "\n",
    "stopwatch = utils.StopWatch()\n",
    "\n",
    "docsize = 70\n",
    "\n",
    "#for aux in lm.auxmap.values():\n",
    "#    print(aux)\n",
    "\n",
    "def listfiles(path):\n",
    "    result_arr = []\n",
    "    filenames = os.listdir(path)\n",
    "    for filename in filenames:\n",
    "        full_filename = os.path.join(path, filename)\n",
    "        result_arr.append(full_filename)\n",
    "    return result_arr\n",
    "    \n",
    "files = listfiles('../samples')\n",
    "files = sorted(files)\n",
    "files.extend( listfiles('../../data/wiki') )\n",
    "files.extend( listfiles('../../data/NP') )\n",
    "\n",
    "print (len(files), \"files\")\n",
    "\n",
    "outputpath = \"../../output/\"\n",
    "\n",
    "model_path = outputpath + \"model/\" + str(docsize)\n",
    "os.makedirs(model_path, exist_ok=True)\n",
    "\n",
    "\n",
    "trainer = tr.Trainer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Texts >>> \n",
      "Load Texts:  3s.682ms. ( 3,681 ms.)\n",
      "Single Count: 12449\n",
      "용언 Count: 2423\n",
      "체언 Count: 7064\n",
      "Ambiguous Count: 1023\n",
      "Build Vocab:  0s.114ms. ( 113 ms.)\n",
      "Word Count :  35,687\n",
      "head Count :  28,185\n",
      "tail Count :  329\n",
      "Saving Model to ../../output/model/70\n",
      "Save  ../../output/model/70/heads.csv   소요시간: 0.084\n",
      "Save  ../../output/model/70/tails.csv   소요시간: 0.006\n"
     ]
    }
   ],
   "source": [
    "\n",
    "stopwatch.start()\n",
    "\n",
    "def extractHeads(words) :\n",
    "    word_texts = []\n",
    "    for word in words:\n",
    "        if word.bestpair:\n",
    "            word_texts.append(word.bestpair.head.text)\n",
    "        else:\n",
    "            word_texts.append(word.text)\n",
    "    return word_texts\n",
    "\n",
    "\n",
    "def extractTails(words) :\n",
    "    word_texts = []\n",
    "    for word in words:\n",
    "        if word.bestpair:\n",
    "            #word_texts.append(word.bestpair.tail.text + \":\" + str(word.bestpair.type))\n",
    "            word_texts.append(word.bestpair.tail.text)\n",
    "        else:\n",
    "            word_texts.append(\"\")\n",
    "            #word_texts.append(word.text)\n",
    "    return word_texts\n",
    "\n",
    "\n",
    "\n",
    "def extractTags(words) :\n",
    "    word_texts = []\n",
    "    for word in words:\n",
    "        if word.bestpair:\n",
    "            word_texts.append(word.bestpair.tags)\n",
    "        else:\n",
    "            word_texts.append(\"*\")\n",
    "    return word_texts\n",
    "\n",
    "\n",
    "\n",
    "def writeTails(outputpath, index, sentence_array):\n",
    "    outputfile = outputpath + 'samples/' + str(index) + '_tails.txt'\n",
    "    \n",
    "    words_array = trainer.scoreDocument(sentence_array)\n",
    "    \n",
    "    with open(outputfile, 'w', encoding='utf-8') as out:\n",
    "        writer = csv.writer(out)\n",
    "        for words in words_array:\n",
    "            word_texts = extractTails(words)\n",
    "            writer.writerow(word_texts)\n",
    "        out.close()\n",
    "        \n",
    "def writeAll(tailswriter, headswriter, tagswriter, sentence_array):\n",
    "    words_array = trainer.scoreDocument(sentence_array)\n",
    "    \n",
    "    for words in words_array:\n",
    "        word_texts = extractTails(words)\n",
    "        \n",
    "        if len(word_texts) > 2:\n",
    "            tailswriter.writerow(word_texts)\n",
    "            heads = extractHeads(words)\n",
    "            headswriter.writerow(heads)\n",
    "            tags = extractTags(words)\n",
    "            tagswriter.writerow(tags)\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "tailsfile = open(outputpath + 'samples/tails.txt', 'w', encoding='utf-8')\n",
    "tailswriter = csv.writer(tailsfile)\n",
    "\n",
    "\n",
    "headsfile = open(outputpath + 'samples/heads.txt', 'w', encoding='utf-8')\n",
    "headswriter = csv.writer(headsfile)\n",
    "\n",
    "\n",
    "tagsfile = open(outputpath + 'samples/tags.txt', 'w', encoding='utf-8')\n",
    "tagswriter = csv.writer(tagsfile)\n",
    "\n",
    "print(\"Loading Texts >>> \")\n",
    "index =0\n",
    "for file in files[:docsize]:\n",
    "    if file.endswith(\".txt\") :\n",
    "        sentence_array, words_array = trainer.loadfile(file, verbose=False)\n",
    "        writeAll(tailswriter,headswriter, tagswriter,sentence_array)\n",
    "        index += 1\n",
    "        if index % 100 == 0:\n",
    "            print(index,end=\">\")\n",
    "            if index % 1000 == 0:\n",
    "                print()\n",
    "            \n",
    "tailsfile.close()\n",
    "tagsfile.close()\n",
    "\n",
    "print(\"Load Texts: \" , stopwatch.secmilli() , \"(\", stopwatch.millisecstr(), \"ms.)\")\n",
    "\n",
    "stopwatch.start()\n",
    "\n",
    "snglist, ylist, clist, ambilist = trainer.buildVocab()\n",
    "\n",
    "print(\"Build Vocab: \" , stopwatch.secmilli() , \"(\", stopwatch.millisecstr(), \"ms.)\")\n",
    "print(\"Word Count : \" , utils.comma(len(trainer.wordmap.words)) )\n",
    "print(\"head Count : \" , utils.comma(len(trainer.wordmap.heads)) )\n",
    "print(\"tail Count : \" , utils.comma(len(trainer.wordmap.tails)) )\n",
    "\n",
    "\n",
    "trainer.savemodel(model_path)\n",
    "\n",
    "with open(outputpath + \"model/ambiguous.csv\", 'w', encoding='utf-8') as csvfile :\n",
    "    writer = csv.writer(csvfile)\n",
    "    for head in ambilist:\n",
    "        writer.writerow([head.text, '+'.join(head.pos), head.tails])\n",
    "            \n",
    "    csvfile.close()\n",
    "\n",
    "with open(outputpath + \"model/single.csv\", 'w', encoding='utf-8') as csvfile :\n",
    "    writer = csv.writer(csvfile)\n",
    "    for head in snglist:\n",
    "        writer.writerow([head.text, '+'.join(head.pos), head.tails])\n",
    "            \n",
    "    csvfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write  ../../output/np_words_70.csv   소요시간: 0s.311ms. ( 311 ms.)\n",
      "Write  ../../output/np_words_scored.csv   소요시간: 0s.253ms. ( 253 ms.)\n"
     ]
    }
   ],
   "source": [
    "def writeWord(writer, word):\n",
    "    if len(word.particles) == 0:\n",
    "        writer.writerow([word.text, 'X'])\n",
    "    for part in word.particles:\n",
    "        h = part.head.text if part.head else ''\n",
    "        t = part.tail.text if part.tail else ''\n",
    "        writer.writerow([word.text, h, t, part.score, part.tags, part.pos])\n",
    "\n",
    "utils.writecsv(\"../../output/np_words_\" + str(docsize) + \".csv\", trainer.wordmap.words.values(), writeWord)\n",
    "\n",
    "def writeWord(writer, word):\n",
    "    bestpair = word.bestpair\n",
    "    if bestpair :\n",
    "        h = bestpair.head.text if bestpair.head else ''\n",
    "        t = bestpair.tail.text if bestpair.tail else 'X'\n",
    "        writer.writerow([ word.text, '   ', h, t, '   ', bestpair.score, bestpair.tags, bestpair.pos ])\n",
    "    else:\n",
    "        writer.writerow([ word.text, 'X' ])\n",
    "        \n",
    "utils.writecsv(\"../../output/np_words_scored.csv\", trainer.wordmap.words.values(), writeWord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write  ../../output/np_tails.csv   소요시간: 0s.012ms. ( 11 ms.)\n",
      "Write  ../../output/np_tails_occ.csv   소요시간: 0s.010ms. ( 9 ms.)\n"
     ]
    }
   ],
   "source": [
    "def writeTails(writer, tail):\n",
    "    #row = [tail.text]\n",
    "    #for head in tail.heads:\n",
    "    #    row.append(head.text)\n",
    "    #row = [tail.text, len(tail.heads), tail.heads]\n",
    "    row = [tail.text, len(tail.heads)]\n",
    "    writer.writerow(row)\n",
    "\n",
    "tailsorted = sorted(trainer.wordmap.tails.values(), key=lambda tail:tail.text[len(tail.text)-1])\n",
    "utils.writecsv(\"../../output/np_tails.csv\", tailsorted, writeTails)\n",
    "\n",
    "\n",
    "tailsorted2 = sorted(trainer.wordmap.tails.values(), key=lambda tail:len(tail.heads), reverse=True)\n",
    "utils.writecsv(\"../../output/np_tails_occ.csv\", tailsorted2, writeTails)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write  ../../output/np_heads.csv   소요시간: 0s.114ms. ( 114 ms.)\n",
      "Write  ../../output/np_heads_occ.csv   소요시간: 0s.124ms. ( 124 ms.)\n",
      "Write  ../../output/np_head_scored.csv   소요시간: 0s.102ms. ( 101 ms.)\n"
     ]
    }
   ],
   "source": [
    "def writeHeads(writer, head):\n",
    "    row = [ str(head), '  ', len(head.tails), ' ']\n",
    "    for tail in head.tails:\n",
    "        row.append(tail.text)\n",
    "    writer.writerow(row)\n",
    "\n",
    "headsorted = sorted(trainer.wordmap.heads.values(), key=lambda head:head.text)\n",
    "utils.writecsv(\"../../output/np_heads.csv\", headsorted, writeHeads)\n",
    "\n",
    "\n",
    "headsorted2 = sorted(trainer.wordmap.heads.values(), key=lambda head:len(head.tails), reverse=True)\n",
    "utils.writecsv(\"../../output/np_heads_occ.csv\", headsorted2, writeHeads)\n",
    "\n",
    "\n",
    "headset = set()\n",
    "\n",
    "for word in trainer.wordmap.words.values():\n",
    "    bestpair = word.bestpair\n",
    "    if bestpair and bestpair.head:\n",
    "        headset.add(bestpair.head)\n",
    "        \n",
    "def writeHead(writer, head):\n",
    "    row = [ str(head), '\\t', len(head.tails), '\\t']\n",
    "    \n",
    "    for tail in head.tails:\n",
    "        row.append(tail.text)\n",
    "        \n",
    "    writer.writerow(row)\n",
    "        \n",
    "\n",
    "        \n",
    "headlist = sorted(headset , key=lambda head:head.text )\n",
    "\n",
    "utils.writecsv(\"../../output/np_head_scored.csv\", headlist, writeHead)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pycor.langmodel as lm\n",
    "\n",
    "def trace(worm, indent):\n",
    "    print(indent, worm) \n",
    "    for p in worm.precedents.values():\n",
    "        for p2 in p:\n",
    "            if p2 == worm:\n",
    "                break\n",
    "            trace(p2, indent+\".\")\n",
    "        \n",
    "# for euls in lm.auxmap.values():\n",
    "#     for eul in euls:\n",
    "#        trace(eul,\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init SentenceParser\n"
     ]
    }
   ],
   "source": [
    "parser = ps.SentenceParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extractWords(words) :\n",
    "    word_texts = []\n",
    "    for word in words:\n",
    "        if word.bestpair:\n",
    "            #word_texts.append(word.bestpair.head.text +\":\"+word.bestpair.tail.text)\n",
    "            pos = word.bestpair.pos if word.bestpair.pos else ''\n",
    "            tag = word.bestpair.tags if word.bestpair.tags else ''\n",
    "#             print(word.bestpair.tail.occurence())\n",
    "            postag = None\n",
    "            if pos or tag:\n",
    "                postag = \"(\"+ str(pos) + \":\" + str(tag) +\")\"\n",
    "            else :\n",
    "                postag = \"\"\n",
    "            word_texts.append(word.bestpair.head.text + postag)\n",
    "        else:\n",
    "            word_texts.append(word.text)\n",
    "    return word_texts\n",
    "\n",
    "def writeExtracted(outputpath, suffix, index, sentence_array):\n",
    "    outputfile = outputpath + 'samples/' + str(index) + suffix + '.txt'\n",
    "    \n",
    "    words_array = trainer.scoreDocument(sentence_array)\n",
    "\n",
    "    with open(outputfile, 'w', encoding='utf-8') as out:\n",
    "        writer = csv.writer(out)\n",
    "        for words in words_array:\n",
    "            word_texts = extractWords(words)\n",
    "            writer.writerow(word_texts)\n",
    "        out.close()\n",
    "        \n",
    "index = 0\n",
    "for file in files[:5]:\n",
    "    if file.endswith(\".txt\") :\n",
    "        sentence_array, words_array = trainer.loadfile(file, verbose=False)\n",
    "        index += 1\n",
    "        writeExtracted(outputpath, '_train', index, sentence_array)\n",
    "\n",
    "\n",
    "index = 0\n",
    "for file in files[:5]:\n",
    "    if file.endswith(\".txt\") :\n",
    "        sentence_array, words_array = parser.loadfile(file, verbose=False)\n",
    "        index += 1\n",
    "        writeExtracted(outputpath, '_parser1', index, sentence_array)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Model from ../../output/model/\n",
      "Load  ../../output/model//heads.csv   소요시간: 0.113\n",
      "Load  ../../output/model//tails.csv   소요시간: 0.008\n"
     ]
    }
   ],
   "source": [
    "parser.loadmodel(outputpath + \"model/\")\n",
    "\n",
    "index = 0\n",
    "for file in files[:5]:\n",
    "    if file.endswith(\".txt\") :\n",
    "        sentence_array, words_array = trainer.loadfile(file, verbose=False)\n",
    "        index += 1\n",
    "        writeExtracted(outputpath, '_parser2', index, sentence_array)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_conda)",
   "language": "python",
   "name": "conda_conda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
