{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Korean Text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init Trainer\n",
      "Init DocResolver\n",
      "3240 files\n",
      "Loading Dictionary from ../samples/dic/dictionary.csv\n",
      "Load  ../samples/dic/dictionary.csv   소요시간: 0.004\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import csv\n",
    "import os\n",
    "from pathlib import Path\n",
    "import sys\n",
    "sys.path.insert(0, '../')\n",
    "import pycor\n",
    "# import pycor.korlang as korlang\n",
    "import pycor.utils as utils\n",
    "import pycor.scoring as scoring\n",
    "from pycor.res import WikiResolver\n",
    "from pycor.res import CollocationResolver\n",
    "\n",
    "stopwatch = utils.StopWatch()\n",
    "\n",
    "docsize = 30\n",
    "\n",
    "#for aux in lm.auxmap.values():\n",
    "#    print(aux)\n",
    "\n",
    "def listfiles(path):\n",
    "    result_arr = []\n",
    "    filenames = os.listdir(path)\n",
    "    for filename in filenames:\n",
    "        full_filename = os.path.join(path, filename)\n",
    "        result_arr.append(full_filename)\n",
    "    return result_arr\n",
    "    \n",
    "files = listfiles('../samples')\n",
    "files = sorted(files)\n",
    "files.extend( listfiles('../../data/wiki2') )\n",
    "files.extend( listfiles('../../data/wiki') )\n",
    "files.extend( listfiles('../../data/NP') )\n",
    "\n",
    "print (len(files), \"files\")\n",
    "\n",
    "outputpath = \"../../output/\" + str(docsize) + \"/\"\n",
    "model_path = outputpath + \"model/\"\n",
    "\n",
    "os.makedirs(outputpath, exist_ok=True)\n",
    "os.makedirs(model_path, exist_ok=True)\n",
    "\n",
    "\n",
    "dictionary_path = \"../samples/dic/dictionary.csv\"\n",
    "\n",
    "# trainer = korlang._trainer\n",
    "\n",
    "# 사전 등록 \n",
    "pycor.loaddic(dictionary_path)\n",
    "# wikiResolver = WikiResolver()\n",
    "# pycor.addresolver(wikiResolver)\n",
    "\n",
    "collocationResolver = CollocationResolver()\n",
    "pycor.addresolver(collocationResolver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Texts >>> \n",
      "Load Texts:  0s.726ms. ( 726 ms.)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "stopwatch.start()\n",
    "\n",
    "def extractHeads(words) :\n",
    "    word_texts = []\n",
    "    for word in words:\n",
    "        if word.bestpair:\n",
    "            word_texts.append(word.bestpair.head.text)\n",
    "        else:\n",
    "            word_texts.append(word.text)\n",
    "    return word_texts\n",
    "\n",
    "\n",
    "def extractTails(words) :\n",
    "    word_texts = []\n",
    "    for word in words:\n",
    "        if word.bestpair:\n",
    "            #word_texts.append(word.bestpair.tail.text + \":\" + str(word.bestpair.type))\n",
    "            word_texts.append(word.bestpair.tail.text)\n",
    "        else:\n",
    "            word_texts.append(\"\")\n",
    "            #word_texts.append(word.text)\n",
    "    return word_texts\n",
    "\n",
    "\n",
    "\n",
    "def extractTags(words) :\n",
    "    word_texts = []\n",
    "    for word in words:\n",
    "        if word.bestpair:\n",
    "            word_texts.append(word.bestpair.tags)\n",
    "        else:\n",
    "            word_texts.append(\"*\")\n",
    "    return word_texts\n",
    "\n",
    "\n",
    "\n",
    "def writeTails(outputpath, index, words_array):\n",
    "    outputfile = outputpath + 'samples/' + str(index) + '_tails.txt'\n",
    "    \n",
    "#     words_array = trainer.scoreDocument(sentence_array)\n",
    "    \n",
    "    with open(outputfile, 'w', encoding='utf-8') as out:\n",
    "        writer = csv.writer(out)\n",
    "        for words in words_array:\n",
    "            word_texts = extractTails(words)\n",
    "            writer.writerow(word_texts)\n",
    "        out.close()\n",
    "        \n",
    "def writeAll(tailswriter, headswriter, tagswriter, words_array):\n",
    "#     words_array = trainer.scoreDocument(sentence_array)\n",
    "    \n",
    "    for words in words_array:\n",
    "        word_texts = extractTails(words)\n",
    "        \n",
    "        if len(word_texts) > 2:\n",
    "            tailswriter.writerow(word_texts)\n",
    "            heads = extractHeads(words)\n",
    "            headswriter.writerow(heads)\n",
    "            tags = extractTags(words)\n",
    "            tagswriter.writerow(tags)\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "tailsfile = open(outputpath + 'tails.txt', 'w', encoding='utf-8')\n",
    "tailswriter = csv.writer(tailsfile)\n",
    "\n",
    "\n",
    "headsfile = open(outputpath + 'heads.txt', 'w', encoding='utf-8')\n",
    "headswriter = csv.writer(headsfile)\n",
    "\n",
    "\n",
    "tagsfile = open(outputpath + 'tags.txt', 'w', encoding='utf-8')\n",
    "tagswriter = csv.writer(tagsfile)\n",
    "\n",
    "print(\"Loading Texts >>> \")\n",
    "index =0\n",
    "for file in files[:docsize]:\n",
    "    if file.endswith(\".txt\") :\n",
    "        _, words_array = pycor.readfile(file)\n",
    "        writeAll(tailswriter,headswriter, tagswriter,words_array)\n",
    "        index += 1\n",
    "        if index % 100 == 0:\n",
    "            print(index,end=\">\")\n",
    "            if index % 1000 == 0:\n",
    "                print()\n",
    "            \n",
    "tailsfile.close()\n",
    "tagsfile.close()\n",
    "\n",
    "print(\"Load Texts: \" , stopwatch.secmilli() , \"(\", stopwatch.millisecstr(), \"ms.)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write  ../../output/30/np_words.csv   소요시간: 0s.080ms. ( 79 ms.)\n",
      "Write  ../../output/30/np_words_scored.csv   소요시간: 0s.033ms. ( 32 ms.)\n"
     ]
    }
   ],
   "source": [
    "def writeWord(writer, word):\n",
    "    if len(word.particles) == 0:\n",
    "        writer.writerow([word.text, 'X'])\n",
    "    for part in word.particles:\n",
    "        h = part.head.text if part.head else ''\n",
    "        t = part.tail.text if part.tail else ''\n",
    "        writer.writerow([word.text, h, t, part.score, part.tags, part.pos])\n",
    "\n",
    "utils.writecsv(outputpath + \"np_words.csv\", pycor.getmodel().words.values(), writeWord)\n",
    "\n",
    "def writeWord(writer, word):\n",
    "    bestpair = word.bestpair\n",
    "    if bestpair :\n",
    "        h = bestpair.head.text if bestpair.head else ''\n",
    "        t = bestpair.tail.text if bestpair.tail else 'X'\n",
    "        writer.writerow([ word.text, '   ', h, t, '   ', bestpair.score, bestpair.tags, bestpair.pos ])\n",
    "    else:\n",
    "        writer.writerow([ word.text, 'X' ])\n",
    "        \n",
    "utils.writecsv(outputpath + \"np_words_scored.csv\", pycor.getmodel().words.values(), writeWord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single Count: 1457\n",
      "용언 Count: 303\n",
      "체언 Count: 1293\n",
      "Ambiguous Count: 270\n",
      "Heads Count: 4620\n",
      "Collocations Count: 68\n",
      "Tails Count: 259\n",
      "Clear words size: 4893\n",
      "Build Vocab:  0s.086ms. ( 86 ms.)\n",
      "Word Count :  0\n",
      "head Count :  4,620\n",
      "tail Count :  259\n",
      "Load  ../../output/30/model//heads.csv   소요시간: 0.028\n",
      "Load  ../../output/30/model//tails.csv   소요시간: 0.007\n",
      "Old Heads: 3296\n",
      "Saving Model to ../../output/30/model/\n",
      "Save  ../../output/30/model//heads.csv   소요시간: 0.029\n",
      "Save  ../../output/30/model//tails.csv   소요시간: 0.006\n",
      "Save  ../../output/30/model//collocations.csv   소요시간: 0.004\n",
      "Clear Heads: 1322 remains.\n"
     ]
    }
   ],
   "source": [
    "import pycor.dev.model as dev\n",
    "\n",
    "stopwatch.start()\n",
    "\n",
    "snglist, ylist, clist, ambilist = pycor.buildvocab()\n",
    "\n",
    "wordmap = pycor.getmodel()\n",
    "\n",
    "print(\"Build Vocab: \" , stopwatch.secmilli() , \"(\", stopwatch.millisecstr(), \"ms.)\")\n",
    "print(\"Word Count : \" , utils.comma(len(wordmap.words)) )\n",
    "print(\"head Count : \" , utils.comma(len(wordmap.heads)) )\n",
    "print(\"tail Count : \" , utils.comma(len(wordmap.tails)) )\n",
    "\n",
    "oldmodel = None\n",
    "\n",
    "try:\n",
    "    oldmodel = dev.loadmodel(model_path)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "if oldmodel:\n",
    "    print(\"Old Heads:\", len(oldmodel.heads))\n",
    "    result = dev.compareHeads(oldmodel.heads, wordmap.heads)\n",
    "    dev.saveCompHeads(model_path+\"/compare.csv\", result)\n",
    "\n",
    "pycor.savemodel(model_path)\n",
    "\n",
    "\n",
    "# with open(model_path + \"ambiguous.csv\", 'w', encoding='utf-8') as csvfile :\n",
    "#     writer = csv.writer(csvfile)\n",
    "#     for head in ambilist:\n",
    "#         writer.writerow([head.text, '+'.join(head.pos), head.tails])\n",
    "            \n",
    "#     csvfile.close()\n",
    "\n",
    "# with open(model_path + \"single.csv\", 'w', encoding='utf-8') as csvfile :\n",
    "#     writer = csv.writer(csvfile)\n",
    "#     for head in snglist:\n",
    "#         writer.writerow([head.text, '+'.join(head.pos), head.tails])\n",
    "            \n",
    "#     csvfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write  ../../output/30/np_tails.csv   소요시간: 0s.008ms. ( 8 ms.)\n",
      "Write  ../../output/30/np_tails_occ.csv   소요시간: 0s.007ms. ( 7 ms.)\n"
     ]
    }
   ],
   "source": [
    "def writeTails(writer, tail):\n",
    "    #row = [tail.text]\n",
    "    #for head in tail.heads:\n",
    "    #    row.append(head.text)\n",
    "    #row = [tail.text, len(tail.heads), tail.heads]\n",
    "    row = [tail.text, len(tail.heads)]\n",
    "    writer.writerow(row)\n",
    "\n",
    "wordmap = pycor.getmodel()\n",
    "tailsorted = sorted(wordmap.tails.values(), key=lambda tail:tail.text[len(tail.text)-1])\n",
    "utils.writecsv(outputpath + \"np_tails.csv\", tailsorted, writeTails)\n",
    "\n",
    "\n",
    "tailsorted2 = sorted(wordmap.tails.values(), key=lambda tail:len(tail.heads), reverse=True)\n",
    "utils.writecsv(outputpath + \"np_tails_occ.csv\", tailsorted2, writeTails)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write  ../../output/30/np_heads.csv   소요시간: 0s.027ms. ( 27 ms.)\n",
      "Write  ../../output/30/np_heads_occ.csv   소요시간: 0s.018ms. ( 17 ms.)\n",
      "Write  ../../output/30/np_head_scored.csv   소요시간: 0s.005ms. ( 5 ms.)\n"
     ]
    }
   ],
   "source": [
    "def writeHeads(writer, head):\n",
    "    row = [ str(head), '  ', len(head.tails), ' ']\n",
    "    for tail in head.tails:\n",
    "        row.append(tail.text)\n",
    "    writer.writerow(row)\n",
    "\n",
    "wordmap = pycor.getmodel()\n",
    "\n",
    "headsorted = sorted(wordmap.heads.values(), key=lambda head:head.text)\n",
    "utils.writecsv(outputpath + \"np_heads.csv\", headsorted, writeHeads)\n",
    "\n",
    "\n",
    "headsorted2 = sorted( wordmap.heads.values(), key=lambda head:len(head.tails), reverse=True)\n",
    "utils.writecsv(outputpath + \"np_heads_occ.csv\", headsorted2, writeHeads)\n",
    "\n",
    "\n",
    "headset = set()\n",
    "\n",
    "for word in  wordmap.words.values():\n",
    "    bestpair = word.bestpair\n",
    "    if bestpair and bestpair.head:\n",
    "        headset.add(bestpair.head)\n",
    "        \n",
    "def writeHead(writer, head):\n",
    "    row = [ str(head), '\\t', len(head.tails), '\\t']\n",
    "    \n",
    "    for tail in head.tails:\n",
    "        row.append(tail.text)\n",
    "        \n",
    "    writer.writerow(row)\n",
    "        \n",
    "\n",
    "        \n",
    "headlist = sorted(headset , key=lambda head:head.text )\n",
    "\n",
    "utils.writecsv(outputpath + \"np_head_scored.csv\", headlist, writeHead)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Model from ../../output/30/model/\n",
      "Load  ../../output/30/model//heads.csv   소요시간: 0.024\n",
      "Load  ../../output/30/model//tails.csv   소요시간: 0.008\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def extractWords(words) :\n",
    "    word_texts = []\n",
    "    for word in words:\n",
    "        if word.bestpair:\n",
    "            #word_texts.append(word.bestpair.head.text +\":\"+word.bestpair.tail.text)\n",
    "            pos = word.bestpair.pos if word.bestpair.pos else ''\n",
    "            tag = word.bestpair.tags if word.bestpair.tags else ''\n",
    "#             print(word.bestpair.tail.occurence())\n",
    "            postag = None\n",
    "            if pos or tag:\n",
    "                postag = \"(\"+ str(pos) + \":\" + str(tag) +\")\"\n",
    "            else :\n",
    "                postag = \"\"\n",
    "            word_texts.append(word.bestpair.head.text + postag)\n",
    "        else:\n",
    "            word_texts.append(word.text)\n",
    "    return word_texts\n",
    "\n",
    "def writeExtracted(outputpath, suffix, index, words_array):\n",
    "    outputfile = outputpath + str(index) + suffix + '.txt'\n",
    "    \n",
    "#     words_array = trainer.scoreDocument(sentence_array)\n",
    "\n",
    "    with open(outputfile, 'w', encoding='utf-8') as out:\n",
    "        writer = csv.writer(out)\n",
    "        for words in words_array:\n",
    "            word_texts = extractWords(words)\n",
    "            writer.writerow(word_texts)\n",
    "        out.close()\n",
    "        \n",
    "index = 0\n",
    "for file in files[:5]:\n",
    "    if file.endswith(\".txt\") :\n",
    "        sentence_array, words_array = pycor.readfile(file)\n",
    "        index += 1\n",
    "        writeExtracted(outputpath, '_train', index, words_array)\n",
    "\n",
    "import pycor\n",
    "pycor.loadmodel(model_path)\n",
    "\n",
    "index = 0\n",
    "for file in files[:5]:\n",
    "    if file.endswith(\".txt\") :\n",
    "        sentence_array, words_array = pycor.readfile(file)\n",
    "        index += 1\n",
    "        writeExtracted(outputpath, '_pycor', index, words_array)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "라브아지에 라브아지에  0.997476279357107 [] set()\n",
      "라브아지에 라브아지 에 0.9241506863049428 ['JKB-TO'] set()\n",
      "라브아지에 라브아지 에 0.9241506863049428 ['JKB-AT'] set()\n",
      "라브아지에::set():0\n",
      "라브아지에:에게:{'JKB-TO'}:13\n",
      "라브아지에가 라브아지에 가 0.9871389101307597 ['JKS'] set()\n",
      "라브아지에가 라브아지 에가 0.9862026690197803 ['JKS', 'JKB-TO'] set()\n",
      "라브아지에가 라브아지에 가 0.9871389101307597 ['JKC'] set()\n",
      "라브아지에:가:{'JKS'}:121\n",
      "라브아지에:를:{'JKO'}:129\n",
      "라브아지에 라브아지에  0.9974833261011852 [] set()\n",
      "라브아지에 라브아지 에 0.9246661917263435 ['JKB-TO'] set()\n",
      "라브아지에 라브아지 에 0.9246661917263435 ['JKB-AT'] set()\n",
      "라브아지에::set():0\n",
      "간디:에:{'JKB-TO', 'MM'}:208\n",
      "간디:에게:{'JKB-TO'}:13\n"
     ]
    }
   ],
   "source": [
    "word = pycor.resolveword('라브아지에', debug=True)\n",
    "print(word.bestpair)\n",
    "word = pycor.resolveword('라브아지에에게')\n",
    "print(word.bestpair)\n",
    "word = pycor.resolveword('라브아지에가', debug=True)\n",
    "print(word.bestpair)\n",
    "word = pycor.resolveword('라브아지에를')\n",
    "print(word.bestpair)\n",
    "word = pycor.resolveword('라브아지에', debug=True)\n",
    "print(word.bestpair)\n",
    "\n",
    "word = pycor.resolveword('간디에')\n",
    "print(word.bestpair)\n",
    "word = pycor.resolveword('간디에게')\n",
    "print(word.bestpair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "감소되:서:{'EC-because'}:100\n",
      "감소되:어서:{'V', 'EC-because'}:5\n",
      "감소되:었다가:{'EC-and', 'EPT-pp'}:2\n",
      "감소됐다가 감소됐 다가 0.6283873785432755 ['EC-and'] set()\n",
      "감소됐다가 감소되 었다가 0.9734442273847255 ['EC-and', 'EPT-pp'] set()\n",
      "감소됐다가 감소됐다 가 0.3118084505070805 ['JKS'] set()\n",
      "감소됐다가 감소됐다 가 0.3118084505070805 ['JKC'] set()\n",
      "감소되:었다가:{'EC-and', 'EPT-pp'}:2\n"
     ]
    }
   ],
   "source": [
    "word = pycor.resolveword('감소되서')\n",
    "print(word.bestpair)\n",
    "word = pycor.resolveword('감소돼서')\n",
    "print(word.bestpair)\n",
    "word = pycor.resolveword('감소되었다가')\n",
    "print(word.bestpair)\n",
    "word = pycor.resolveword('감소됐다가', debug=True)\n",
    "print(word.bestpair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_conda)",
   "language": "python",
   "name": "conda_conda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
