{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Korean Text\n",
    "* from korean wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import csv\n",
    "import os\n",
    "from pathlib import Path\n",
    "import sys\n",
    "sys.path.insert(0, '../')\n",
    "import pycor.korlang as korlang\n",
    "import pycor.utils as utils\n",
    "\n",
    "stopwatch = utils.StopWatch()\n",
    "\n",
    "docsize = 70\n",
    "\n",
    "#for aux in lm.auxmap.values():\n",
    "#    print(aux)\n",
    "\n",
    "def listfiles(path):\n",
    "    result_arr = []\n",
    "    filenames = os.listdir(path)\n",
    "    for filename in filenames:\n",
    "        full_filename = os.path.join(path, filename)\n",
    "        result_arr.append(full_filename)\n",
    "    return result_arr\n",
    "    \n",
    "files = listfiles('../samples')\n",
    "files = sorted(files)\n",
    "files.extend( listfiles('../../data/wiki') )\n",
    "files.extend( listfiles('../../data/NP') )\n",
    "\n",
    "print (len(files), \"files\")\n",
    "\n",
    "outputpath = \"../../output/\" + str(docsize) + \"/\"\n",
    "model_path = outputpath + \"model/\"\n",
    "\n",
    "os.makedirs(outputpath, exist_ok=True)\n",
    "os.makedirs(model_path, exist_ok=True)\n",
    "\n",
    "\n",
    "trainer = korlang._trainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "stopwatch.start()\n",
    "\n",
    "def extractHeads(words) :\n",
    "    word_texts = []\n",
    "    for word in words:\n",
    "        if word.bestpair:\n",
    "            word_texts.append(word.bestpair.head.text)\n",
    "        else:\n",
    "            word_texts.append(word.text)\n",
    "    return word_texts\n",
    "\n",
    "\n",
    "def extractTails(words) :\n",
    "    word_texts = []\n",
    "    for word in words:\n",
    "        if word.bestpair:\n",
    "            #word_texts.append(word.bestpair.tail.text + \":\" + str(word.bestpair.type))\n",
    "            word_texts.append(word.bestpair.tail.text)\n",
    "        else:\n",
    "            word_texts.append(\"\")\n",
    "            #word_texts.append(word.text)\n",
    "    return word_texts\n",
    "\n",
    "\n",
    "\n",
    "def extractTags(words) :\n",
    "    word_texts = []\n",
    "    for word in words:\n",
    "        if word.bestpair:\n",
    "            word_texts.append(word.bestpair.tags)\n",
    "        else:\n",
    "            word_texts.append(\"*\")\n",
    "    return word_texts\n",
    "\n",
    "\n",
    "\n",
    "def writeTails(outputpath, index, sentence_array):\n",
    "    outputfile = outputpath + 'samples/' + str(index) + '_tails.txt'\n",
    "    \n",
    "    words_array = trainer.scoreDocument(sentence_array)\n",
    "    \n",
    "    with open(outputfile, 'w', encoding='utf-8') as out:\n",
    "        writer = csv.writer(out)\n",
    "        for words in words_array:\n",
    "            word_texts = extractTails(words)\n",
    "            writer.writerow(word_texts)\n",
    "        out.close()\n",
    "        \n",
    "def writeAll(tailswriter, headswriter, tagswriter, sentence_array):\n",
    "    words_array = trainer.scoreDocument(sentence_array)\n",
    "    \n",
    "    for words in words_array:\n",
    "        word_texts = extractTails(words)\n",
    "        \n",
    "        if len(word_texts) > 2:\n",
    "            tailswriter.writerow(word_texts)\n",
    "            heads = extractHeads(words)\n",
    "            headswriter.writerow(heads)\n",
    "            tags = extractTags(words)\n",
    "            tagswriter.writerow(tags)\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "tailsfile = open(outputpath + 'tails.txt', 'w', encoding='utf-8')\n",
    "tailswriter = csv.writer(tailsfile)\n",
    "\n",
    "\n",
    "headsfile = open(outputpath + 'heads.txt', 'w', encoding='utf-8')\n",
    "headswriter = csv.writer(headsfile)\n",
    "\n",
    "\n",
    "tagsfile = open(outputpath + 'tags.txt', 'w', encoding='utf-8')\n",
    "tagswriter = csv.writer(tagsfile)\n",
    "\n",
    "print(\"Loading Texts >>> \")\n",
    "index =0\n",
    "for file in files[:docsize]:\n",
    "    if file.endswith(\".txt\") :\n",
    "        sentence_array, words_array = trainer.loadfile(file)\n",
    "        writeAll(tailswriter,headswriter, tagswriter,sentence_array)\n",
    "        index += 1\n",
    "        if index % 100 == 0:\n",
    "            print(index,end=\">\")\n",
    "            if index % 1000 == 0:\n",
    "                print()\n",
    "            \n",
    "tailsfile.close()\n",
    "tagsfile.close()\n",
    "\n",
    "print(\"Load Texts: \" , stopwatch.secmilli() , \"(\", stopwatch.millisecstr(), \"ms.)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "stopwatch.start()\n",
    "\n",
    "print(\"Build Vocab: \" , stopwatch.secmilli() , \"(\", stopwatch.millisecstr(), \"ms.)\")\n",
    "print(\"Word Count : \" , utils.comma(len(trainer.wordmap.words)) )\n",
    "print(\"head Count : \" , utils.comma(len(trainer.wordmap.heads)) )\n",
    "print(\"tail Count : \" , utils.comma(len(trainer.wordmap.tails)) )\n",
    "\n",
    "\n",
    "\n",
    "snglist, ylist, clist, ambilist = trainer.buildVocab()\n",
    "\n",
    "\n",
    "trainer.savemodel(model_path)\n",
    "\n",
    "with open(model_path + \"ambiguous.csv\", 'w', encoding='utf-8') as csvfile :\n",
    "    writer = csv.writer(csvfile)\n",
    "    for head in ambilist:\n",
    "        writer.writerow([head.text, '+'.join(head.pos), head.tails])\n",
    "            \n",
    "    csvfile.close()\n",
    "\n",
    "with open(model_path + \"single.csv\", 'w', encoding='utf-8') as csvfile :\n",
    "    writer = csv.writer(csvfile)\n",
    "    for head in snglist:\n",
    "        writer.writerow([head.text, '+'.join(head.pos), head.tails])\n",
    "            \n",
    "    csvfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def writeWord(writer, word):\n",
    "    if len(word.particles) == 0:\n",
    "        writer.writerow([word.text, 'X'])\n",
    "    for part in word.particles:\n",
    "        h = part.head.text if part.head else ''\n",
    "        t = part.tail.text if part.tail else ''\n",
    "        writer.writerow([word.text, h, t, part.score, part.tags, part.pos])\n",
    "\n",
    "utils.writecsv(outputpath + \"np_words.csv\", trainer.wordmap.words.values(), writeWord)\n",
    "\n",
    "def writeWord(writer, word):\n",
    "    bestpair = word.bestpair\n",
    "    if bestpair :\n",
    "        h = bestpair.head.text if bestpair.head else ''\n",
    "        t = bestpair.tail.text if bestpair.tail else 'X'\n",
    "        writer.writerow([ word.text, '   ', h, t, '   ', bestpair.score, bestpair.tags, bestpair.pos ])\n",
    "    else:\n",
    "        writer.writerow([ word.text, 'X' ])\n",
    "        \n",
    "utils.writecsv(outputpath + \"np_words_scored.csv\", trainer.wordmap.words.values(), writeWord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def writeTails(writer, tail):\n",
    "    #row = [tail.text]\n",
    "    #for head in tail.heads:\n",
    "    #    row.append(head.text)\n",
    "    #row = [tail.text, len(tail.heads), tail.heads]\n",
    "    row = [tail.text, len(tail.heads)]\n",
    "    writer.writerow(row)\n",
    "\n",
    "tailsorted = sorted(trainer.wordmap.tails.values(), key=lambda tail:tail.text[len(tail.text)-1])\n",
    "utils.writecsv(outputpath + \"np_tails.csv\", tailsorted, writeTails)\n",
    "\n",
    "\n",
    "tailsorted2 = sorted(trainer.wordmap.tails.values(), key=lambda tail:len(tail.heads), reverse=True)\n",
    "utils.writecsv(outputpath + \"np_tails_occ.csv\", tailsorted2, writeTails)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def writeHeads(writer, head):\n",
    "    row = [ str(head), '  ', len(head.tails), ' ']\n",
    "    for tail in head.tails:\n",
    "        row.append(tail.text)\n",
    "    writer.writerow(row)\n",
    "\n",
    "headsorted = sorted(trainer.wordmap.heads.values(), key=lambda head:head.text)\n",
    "utils.writecsv(outputpath + \"np_heads.csv\", headsorted, writeHeads)\n",
    "\n",
    "\n",
    "headsorted2 = sorted(trainer.wordmap.heads.values(), key=lambda head:len(head.tails), reverse=True)\n",
    "utils.writecsv(outputpath + \"np_heads_occ.csv\", headsorted2, writeHeads)\n",
    "\n",
    "\n",
    "headset = set()\n",
    "\n",
    "for word in trainer.wordmap.words.values():\n",
    "    bestpair = word.bestpair\n",
    "    if bestpair and bestpair.head:\n",
    "        headset.add(bestpair.head)\n",
    "        \n",
    "def writeHead(writer, head):\n",
    "    row = [ str(head), '\\t', len(head.tails), '\\t']\n",
    "    \n",
    "    for tail in head.tails:\n",
    "        row.append(tail.text)\n",
    "        \n",
    "    writer.writerow(row)\n",
    "        \n",
    "\n",
    "        \n",
    "headlist = sorted(headset , key=lambda head:head.text )\n",
    "\n",
    "utils.writecsv(outputpath + \"np_head_scored.csv\", headlist, writeHead)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pycor.langmodel as lm\n",
    "\n",
    "def trace(worm, indent):\n",
    "    print(indent, worm) \n",
    "    for p in worm.precedents.values():\n",
    "        for p2 in p:\n",
    "            if p2 == worm:\n",
    "                break\n",
    "            trace(p2, indent+\".\")\n",
    "        \n",
    "# for euls in lm.auxmap.values():\n",
    "#     for eul in euls:\n",
    "#        trace(eul,\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extractWords(words) :\n",
    "    word_texts = []\n",
    "    for word in words:\n",
    "        if word.bestpair:\n",
    "            #word_texts.append(word.bestpair.head.text +\":\"+word.bestpair.tail.text)\n",
    "            pos = word.bestpair.pos if word.bestpair.pos else ''\n",
    "            tag = word.bestpair.tags if word.bestpair.tags else ''\n",
    "#             print(word.bestpair.tail.occurence())\n",
    "            postag = None\n",
    "            if pos or tag:\n",
    "                postag = \"(\"+ str(pos) + \":\" + str(tag) +\")\"\n",
    "            else :\n",
    "                postag = \"\"\n",
    "            word_texts.append(word.bestpair.head.text + postag)\n",
    "        else:\n",
    "            word_texts.append(word.text)\n",
    "    return word_texts\n",
    "\n",
    "def writeExtracted(outputpath, suffix, index, sentence_array):\n",
    "    outputfile = outputpath + str(index) + suffix + '.txt'\n",
    "    \n",
    "    words_array = trainer.scoreDocument(sentence_array)\n",
    "\n",
    "    with open(outputfile, 'w', encoding='utf-8') as out:\n",
    "        writer = csv.writer(out)\n",
    "        for words in words_array:\n",
    "            word_texts = extractWords(words)\n",
    "            writer.writerow(word_texts)\n",
    "        out.close()\n",
    "        \n",
    "index = 0\n",
    "for file in files[:5]:\n",
    "    if file.endswith(\".txt\") :\n",
    "        sentence_array, words_array = trainer.loadfile(file)\n",
    "        index += 1\n",
    "        writeExtracted(outputpath, '_train', index, sentence_array)\n",
    "\n",
    "import pycor\n",
    "pycor.loadmodel(model_path)\n",
    "\n",
    "index = 0\n",
    "for file in files[:5]:\n",
    "    if file.endswith(\".txt\") :\n",
    "        sentence_array, words_array = pycor.readfile(file)\n",
    "        index += 1\n",
    "        writeExtracted(outputpath, '_pycor', index, sentence_array)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_conda)",
   "language": "python",
   "name": "conda_conda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
